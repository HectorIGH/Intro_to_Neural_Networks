# -*- coding: utf-8 -*-
"""HW04_CNN_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pfNs3cAHOjciXPFsjuTXcy7V1dWk_X6-
"""

import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense,Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
from keras.callbacks import ReduceLROnPlateau
from keras.utils import to_categorical, plot_model, model_to_dot, print_summary
from keras.preprocessing.image import ImageDataGenerator

from IPython.display import SVG

import matplotlib.pyplot as plt

from google.colab import files

batch_size = 86
num_classes = 10
epochs = 30
# Images dimensions
img_rows, img_cols = 28, 28

# Loading the data and split in training and test
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Showing the first image
plt.imshow(X_train[0])

# Reshaping the data to fit the model

if K.image_data_format() == 'channel_firs':
  X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
  X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)
  input_shape = (1, img_rows, img_cols)
else:
  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
  X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
  input_shape = (img_rows, img_cols, 1)

# Normalizing the data to train faster

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
#print(f'x_train shape: {X_train.shape}')
#print(f'{X_train.shape[0]} train samples')
#print(f'{X_test.shape[0]} test samples')

# Converting class vectors to binary class. One-hot encode target column
Y_train = to_categorical(Y_train, num_classes)
Y_test = to_categorical(Y_test, num_classes)
Y_train[0]

# Building the model

model = Sequential()

model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', input_shape = (img_rows, img_cols,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation = "relu"))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation = "softmax"))

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(X_train)

# Compiling the model

# Modifying the optimizer
optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)

model.compile(loss = keras.losses.categorical_crossentropy, optimizer = optimizer, metrics = ['accuracy'])

#model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adadelta(), metrics = ['accuracy'])

# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)

# Training the model
#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (X_test, Y_test))
history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size = batch_size), epochs = epochs, verbose = 1, validation_data = (X_test, Y_test), callbacks = [learning_rate_reduction])

# Evaluating the model
score = model.evaluate(X_test, Y_test, verbose = 0)
print(f'Test loss: {score[0]}')
print(f'Test accuracy: {score[1]}')

# Plot the model
pltmodel = plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB')
pltmodel

#open('arquitectura_cnn.png', 'wb').write(pltmodel.data)
#files.download("arquitectura_cnn.png")

# Summary of the model
print_summary(model)

# Plot training and validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc = 'upper left')
plt.figure(num=None, figsize=(18, 16), dpi=80, facecolor='w', edgecolor='k')
plt.show()

# Plot training and validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc = 'upper left')
plt.figure(num=None, figsize=(18, 16), dpi=80, facecolor='w', edgecolor='k')
plt.show()

# Plot training loss and accuracy values
plt.plot(history.history['loss'])
plt.plot(history.history['acc'])
plt.title('Model loss and accuracy in training')
plt.ylabel('Values')
plt.xlabel('Epoch')
plt.legend(['Loss', 'Accuracy'], loc = 'upper right')
plt.figure(num=None, figsize=(18, 16), dpi=80, facecolor='w', edgecolor='k')
plt.show()

# Plot training and validation loss values
plt.plot(history.history['val_loss'])
plt.plot(history.history['val_acc'])
plt.title('Model loss and accuracy in validation')
plt.ylabel('Values')
plt.xlabel('Epoch')
plt.legend(['Loss', 'Accuracy'], loc = 'upper left')
plt.figure(num=None, figsize=(18, 16), dpi=80, facecolor='w', edgecolor='k')
plt.show()

